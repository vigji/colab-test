{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphing anatomies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will have a look at how to morph anatomies within a common space, and how to morph them to an atlas. We will also have a look at `napari`, a useful tool for visualizing images and stacks within Python.\n",
    "\n",
    "Before starting, make sure you pip install the following new libraries to the zenenv environment.\n",
    "\n",
    "Close the notebook, shut down jupyter. From the same anaconda environment, run:\n",
    "```\n",
    "pip install napari[pyqt5]\n",
    "pip install dipy\n",
    "pip install bg-atlasapi\n",
    "```\n",
    "\n",
    "Now you should be ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import napari\n",
    "from matplotlib import Path\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "First, load all the anatomies. The stacks have a resolution of `2.0×1.1×1.1 um`. After loading them, we will downsample to a final resolution of 3 um (in an real world data pipeline, you should probably try not to downsample)\n",
    "To downsample, use the `zoom` function from `scipy.ndimage`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anatomies_folder = Path(r\"...\")\n",
    "anatomy_files = sorted(anatomies_folder.glob(\"ana*\"))\n",
    "\n",
    "stack_res = (2, 1.1, 1.1)  # resolution of anatomy stacks\n",
    "final_res = (3, 3, 3)  # target resolution of our resampling\n",
    "anatomies = []\n",
    "# Load the tiffs and use the zoom function to convert them to an isotropic resolution of (3, 3, 3) um\n",
    "for single_file in anatomy_files:\n",
    "    anatomy = tifffile.imread(str(single_file))\n",
    "    anatomies.append(zoom(anatomy, (s / s_f for s, s_f in zip(stack_res, final_res))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at them using napari!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to run this magi command for napari to run smoothly from jupyter notebook. \n",
    "# It is enough to call it one time in the notebook!\n",
    "%gui qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = napari.view_image(anatomies[0])\n",
    "for anat in anatomies[1:]:\n",
    "    v.add_image(anat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registration\n",
    "\n",
    "Finding the correct affine transformation for morphing a stack is an optimization problem: we need to find the best 12 entries for the affine transformation matrix (the last row is fixed, only zeros and final one) that maximise some similarity score (many could be used) between our transformed image and the reference we are morphing it to.\n",
    "\n",
    "Since the parameter space is quite large, and many local minima can be found. Moreover, the data volume is very big. This is why we generally run multiple steps for the morphing procedure, where we first try a coarse registration of a binned/smoothed version of the stack, and then we tinker with the fine parameters. In the same way, we often first compute rigid transformations, and then we allow for deformations to be introduced for better fine matching. \n",
    "\n",
    "To morph data in a reference space, we will follow a 2 steps procedure:\n",
    "\n",
    "1) First, we register all brains of the experiment onto the same representative brain from that experiment. In this way, we can look at all the experiment data within the same \"space\". For this, we will use an affine transformation.\n",
    "\n",
    "2) Then, we will morph the average brain obtained in this \"experiment space\" onto the Max Planck Institute Zebrafish brain reference; here, we will use a non-affine (deforming) transformation.\n",
    "\n",
    "For the registration, we will use the [dipy](https://dipy.org) library. Just for you to know, other options exist, such as [AntsPy](https://antspy.readthedocs.io/en/latest/) or [brainreg](https://github.com/brainglobe/brainreg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within-experiment registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register all brains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will run the affine registration to map all brains to the same space. For time reasons, you will have to run only one registration; we will provide you with the computed affine matrices for the others!\n",
    "\n",
    "Registration procedures can be a bit of a pain, and there is a whole world to be explored behind them. If you want to have a more thorough overview of how dipy works, they have good [tutorials](https://dipy.org/documentation/1.3.0./examples_built/affine_registration_3d/#example-affine-registration-3d) on their functions. For this notebook, we will provide you most of the registration code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define reference for internal registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to arbitrarily define which onto which one of our fish brains we will  morph all the others. Here, we pick the first one; in general, just choose the one that is centered the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = anatomies[0]\n",
    "\n",
    "# Data-to-world transformation matrices. Dipy allow us to specify for each stack a \"trasform_to_world\" matrix\n",
    "# where we would put pre-existing information about how the anatomical stacks relate to \"real world\",\n",
    "# e.g. if they had different resolutions, or origin offsets. In our case, resolution is always the same, and \n",
    "# offsets are something that we need to figure out; so we'll leave it to identity matrices:\n",
    "reference2world = np.eye(4)\n",
    "moving2world = np.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.align.imaffine import MutualInformationMetric, AffineRegistration\n",
    "from dipy.align.transforms import (TranslationTransform3D,\n",
    "                                   RigidTransform3D,\n",
    "                                   AffineTransform3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This defines a \"metric\" that can be used to give a number to \"how similar 2 brains are\". \n",
    "# There are multiple ways of doing this. Here, we are using mutual information:\n",
    "nbins = 32\n",
    "metric = MutualInformationMetric(nbins, None)\n",
    "\n",
    "\n",
    "# As we said, we will use three different scales for calculating more and more precise transformations.\n",
    "# For each one of those levels, we specify:\n",
    "\n",
    "# 1) a sigma function that will blur the stack of some amount, to get rid of high resolution features:\n",
    "sigmas = [10.0, 5.0, 2.0]\n",
    "\n",
    "# 2) a downsampling factor, as for coarse transformations after blurring the stack \n",
    "# we won't miss much info if we downsample:\n",
    "factors = [10, 5, 1]\n",
    "\n",
    "# 3) A maximum number of iteration for the morphing algorithm to converge to an optimum:\n",
    "level_iters = [1000, 100, 50]\n",
    "\n",
    "# Finally, we put all this info in an object that keep together our parameters:\n",
    "affreg = AffineRegistration(metric=metric,\n",
    "                            level_iters=level_iters,\n",
    "                            sigmas=sigmas,\n",
    "                            factors=factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 3 consecutive steps in the morphing:\n",
    "1) First, we just find the best coarse traslation to match the two anatomies (**translation transformation**).\n",
    "\n",
    "2) Second, we try to rotate the moving brain without deforming it, to improve the matching (**rigid transformation**).\n",
    "\n",
    "3) Finally, we allow for the moving brain to be stretched to refine the matching (**affine transformation**).\n",
    "\n",
    "The result of each step will be the starting point for the next transformation.\n",
    "Note that each one of those 3 steps will run at three different spatial scales, as defined in the `AffineRegistration` object above!\n",
    "\n",
    "This procedure can take some time. Here, ideally, it should be some minutes. For a full resolution stack and different registration parameters, it can easily get up to some hours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# The reference anatomy won't need to be morphed\n",
    "anatomies_exp_space = [reference]\n",
    "\n",
    "# Transformation matrix for the reference brain will be just identity matrix:\n",
    "transf_matrices_to_exp = [np.eye(4)]\n",
    "\n",
    "# Loop over all the anatomies but the first one that we used as reference:\n",
    "for moving in anatomies[1:]:\n",
    "    # Translation step:\n",
    "    translation = affreg.optimize(reference, moving, TranslationTransform3D(), None,\n",
    "                                      reference2world, moving2world\n",
    "                                      )\n",
    "\n",
    "    # Rigid transformation step:\n",
    "    rigid = affreg.optimize(reference, moving, RigidTransform3D(), None,\n",
    "                                      reference2world, moving2world,\n",
    "                                      starting_affine=translation.affine)\n",
    "\n",
    "    # Affine transformation step:\n",
    "    affine = affreg.optimize(reference, moving, AffineTransform3D(), None,\n",
    "                                      reference2world, moving2world,\n",
    "                                      starting_affine=rigid.affine)\n",
    "\n",
    "\n",
    "    # Calculate morphed stack, and append to list:\n",
    "    anatomies_exp_space.append(affine.transform(moving))\n",
    "\n",
    "    # Save the transformation matrix (for morphing the coordinates we need the \"affine_inv\" \n",
    "    # attribute of the final transformation object):\n",
    "    transf_matrices_to_exp.append(affine.affine_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use napari to have a look at the morphed stacks one by one! We colored them differently and use additive blending to have them overlapping in the viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = napari.view_image(anatomies_exp_space[0], colormap=\"green\")\n",
    "v.add_image(anatomies_exp_space[1], colormap=\"magenta\", blending=\"additive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results won't be perfect. For your real world data, there generally has to be some tinkering with the transformation parameters; importantly, often we would use a non-affine transformation to improve the transform (that can warp the stack, instead of just shearing it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute average anatomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we average all the anatomies togheter, to create a general anatomy for the experiment where fish to fish variability is averaged out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_anatomy = np.nanmean(np.stack(anatomies_exp_space), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "napari.view_image(super_anatomy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morph to a brain reference using BrainGlobe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will morph our super anatomy onto the [Max Planck Institute](https://fishatlas.neuro.mpg.de) zebrafish atlas. In this way, we can have our data (eg, ROI locations) in a space that contains annotations for all regions of the brain, and we can look at activity within different brain regions. Moreover, morph data on common reference spaces facilitates a lot sharing of the data between different labs!\n",
    "\n",
    "To download the reference brain, we will rely on the [BrainGlobe atlas API](https://github.com/brainglobe/bg-atlasapi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and downsample the reference stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bg_atlasapi.bg_atlas import BrainGlobeAtlas\n",
    "\n",
    "# This will automatically trigger the download of the zebrafish atlas. This atlas has a resolution of 1 um\n",
    "atlas = BrainGlobeAtlas(\"mpin_zfish_1um\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the registration faster, we downsample also the atlas to the final resolution of 3 um:\n",
    "\n",
    "atlas_down = zoom(atlas.reference, (s/f_s for s, f_s in zip(atlas.resolution, final_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match reference and anatomy orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the transformation, we ned to make sure that the anatomical stacks have the same orientation.\n",
    "There are many ways of doing this; it is usually an annoying procedure that requires a lot of arbitrary dimension swapping and flipping. To facilitate things, we will use the [bg-space](https://github.com/brainglobe/bg-space) package. Have a look at the readme there before continuing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the AnatomicalSpace class to map the data onto the Brainglobe anatomical space. \n",
    "# Try to figure out the orientation from the bg-space tutorial! \n",
    "# The origin location for the reference can be found in atlas.space.origin; \n",
    "# the resolution is 3 um after downsampling.\n",
    "\n",
    "# Use napari to check if the two stacks have the same orientation! (they won't be aligned yet of course)\n",
    "from bg_space import AnatomicalSpace \n",
    "\n",
    "sc_atlas = AnatomicalSpace(\"asr\", resolution=final_res) \n",
    "sc_mydata = AnatomicalSpace(\"slp\", resolution=final_res, shape=super_anatomy.shape)\n",
    "\n",
    "# Use the map_stack_to method:\n",
    "mydata_bg_space = sc_mydata.map_stack_to(sc_atlas, super_anatomy)\n",
    "# mydata_bg_space_z = mydata_bg_space / 50\n",
    "\n",
    "v = napari.view_image(atlas_down, colormap=\"green\")\n",
    "v.add_image(mydata_bg_space, colormap=\"magenta\", blending=\"additive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affine transformation\n",
    "If, like in this case, the stacks are in very different spaces, before starting the actual affine transformation it is useful to run a first simple alignment that just align the two stacks using their center of mass.\n",
    "\n",
    "Then, we calculate the affine transformation as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dipy.align.imaffine import transform_centers_of_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Additional step: center of mass\n",
    "c_of_mass = transform_centers_of_mass(atlas_down, reference2world,\n",
    "                                      mydata_bg_space, moving2world)\n",
    "\n",
    "# Translation: \n",
    "translation = affreg.optimize(atlas_down, mydata_bg_space, TranslationTransform3D(), None,\n",
    "                              reference2world, moving2world,\n",
    "                             starting_affine=c_of_mass.affine)\n",
    "\n",
    "# Rigid transformation:\n",
    "rigid = affreg.optimize(atlas_down, mydata_bg_space, RigidTransform3D(), None,\n",
    "                        reference2world, moving2world,\n",
    "                        starting_affine=translation.affine)\n",
    "\n",
    "# Affine transformation. Could be needed, does not change the results much here, and takes long time:\n",
    "# affine = affreg.optimize(atlas_down, mydata_bg_space, AffineTransform3D(), None,\n",
    "#                          reference2world, moving2world,\n",
    "#                         starting_affine=rigid.affine)\n",
    "\n",
    "# Finally, transform the stack:\n",
    "morphed_super_anatomy = rigid.transform(mydata_bg_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results using napari:\n",
    "\n",
    "v = napari.view_image(morphed_super_anatomy, colormap=\"green\")\n",
    "v.add_image(atlas_down, colormap=\"magenta\", blending=\"additive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non affine transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the quality of the morphing to the atlas reference space, we could add after the affine transformation a step where we use an algorithm that can deform locally the stack to make it matching better the reference brain (see [here](https://dipy.org/documentation/1.3.0./examples_built/syn_registration_2d/#example-syn-registration-2d) for an introduction).\n",
    "\n",
    "We won't do it here, for time purposes and because dipy does not allow us to use a non-affine transformation to convert coordinates of extracted ROIs. In the lab, we use AntsPy for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final morphing of anatomies in reference space\n",
    "\n",
    "The final step is now to bring the anatomy stacks that we loaded at the beginning of the notebook all the way into the atlas reference space. For doing this, we won't need to calculate any further registration! We already have the transformation from each stack to the \"experiment reference\", and from the experiment reference to the atlas space. We just need to transform the stacks that we already morphed in the experiment space. Remember, we need to include all the steps that we followed, including the reorientations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "anatomies_atlas_space = []\n",
    "for anatomy_exp_space in anatomies_exp_space:\n",
    "    # Reorient using bg_space:\n",
    "    reoriented = sc_mydata.map_stack_to(sc_atlas, anatomy_exp_space)\n",
    "    \n",
    "    # Affine transformation to atlas space:\n",
    "    affine_trasf = rigid.transform(reoriented)\n",
    "    \n",
    "    # Nonaffine transform:\n",
    "    # anatomy_atlas_space = nonaffine_mapping.transform(affine_trasf)\n",
    "    \n",
    "    anatomies_atlas_space.append(affine_trasf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = napari.view_image(atlas_down, colormap=\"gray\")\n",
    "v.add_image(anatomies_atlas_space[0], colormap=\"magenta\", blending=\"additive\")\n",
    "v.add_image(anatomies_atlas_space[1], colormap=\"blue\", blending=\"additive\")\n",
    "v.add_image(anatomies_atlas_space[2], colormap=\"yellow\", blending=\"additive\")\n",
    "v.add_image(anatomies_atlas_space[3], colormap=\"green\", blending=\"additive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morph coordinates in reference space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have found a suitable transformation, we can apply it to coordinates as well. dipy does not allow for morphing coordinates with a non-affine transformations, while other libraries such as antspy provide that function as well.\n",
    "\n",
    "Therefore, we will only morph the coordinates using the affine transformation that we got.\n",
    "\n",
    "Together with the anatomies, you should have downloaded npy files with the coordinates of ROIs detected in each one of the anatomical stacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the coordinates:\n",
    "original_coords = [np.load(f) for f in sorted(anatomies_folder.glob(\"*.npy\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To morph the coordinates we need to make sure we include all steps that we used to go from the original stacks to the stacks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow coordinate transformation step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "coords = original_coords[i].copy()\n",
    "affine_to_exp = transf_matrices_to_exp[i]\n",
    "anat_exp_space = anatomies_exp_space[i]\n",
    "anatomy = anatomies[i]\n",
    "anat_atlas_space = anatomies_atlas_space[i]\n",
    "\n",
    "\n",
    "# match stack size \n",
    "for i in range(3):\n",
    "    coords[i, :] = coords[i, :] * (stack_res[i] / final_res[i])\n",
    "    \n",
    "coords = np.concatenate([coords, np.ones((1, coords.shape[1]))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original space:\n",
    "v = napari.view_image(anatomy)\n",
    "v.add_points(coords[:3, :].T, size=2, n_dimensional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment space:\n",
    "v = napari.view_image(anat_exp_space)\n",
    "v.add_points((affine_to_exp @ coords)[:3, :].T, size=2, n_dimensional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved to atlas orientation:\n",
    "aff_to_atlas_sp = sc_mydata.transformation_matrix_to(sc_atlas)\n",
    "v = napari.view_image(sc_mydata.map_stack_to(sc_atlas, anat_exp_space))\n",
    "v.add_points((aff_to_atlas_sp @ affine_to_exp @ coords)[:3, :].T, size=2, n_dimensional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In atlas space\n",
    "v = napari.view_image(atlas_down, colormap=\"green\")\n",
    "v.add_image(anat_atlas_space, colormap=\"magenta\", blending=\"additive\")\n",
    "v.add_points((rigid.affine_inv @ aff_to_atlas_sp @ affine_to_exp @ coords)[:3, :].T, \n",
    "             size=2, n_dimensional=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform coordinates for all fish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the code you just saw to transform in a loop the coordinates from all 4 experiments and visualize them with Napari!\n",
    "morphed_coords = []\n",
    "for i, (affine_to_exp, coords) in enumerate(zip(transf_matrices_to_exp, original_coords)):\n",
    "    \n",
    "    # match stack size and append ones:\n",
    "    for i in range(3):\n",
    "        coords[i, :] = coords[i, :] * (stack_res[i] / final_res[i])\n",
    "\n",
    "    coords = np.concatenate([coords, np.ones((1, coords.shape[1]))], axis=0)\n",
    "\n",
    "\n",
    "    coords_atlas_space = (rigid.affine_inv @ aff_to_atlas_sp @ affine_to_exp @ coords)[:3, :].T\n",
    "    morphed_coords.append(coords_atlas_space)\n",
    "    \n",
    "    # save\n",
    "    np.save(anatomies_folder / f\"morphed_coords{i}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = napari.view_image(anat_atlas_space, colormap=\"gray\", blending=\"additive\")\n",
    "v.add_points(morphed_coords[0], \n",
    "             size=2, n_dimensional=True, face_color=\"red\")\n",
    "v.add_points(morphed_coords[1], \n",
    "             size=2, n_dimensional=True, face_color=\"blue\")\n",
    "v.add_points(morphed_coords[2], \n",
    "             size=2, n_dimensional=True, face_color=\"green\")\n",
    "v.add_points(morphed_coords[3], \n",
    "             size=2, n_dimensional=True, face_color=\"yellow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zenenv",
   "language": "python",
   "name": "zenenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
